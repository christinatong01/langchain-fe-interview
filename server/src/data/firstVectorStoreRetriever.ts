export const trace = {
  id: "b9ae5860-0e6b-4963-9471-e742d8078cba",
  inputs: {
    query: "Traduction technique de MessagesState en fran√ßais",
  },
  outputs: {
    documents: [
      {
        metadata: {
          source:
            "https://langchain-ai.github.io/langgraph/concepts/low_level/",
          description: "",
          language: "",
          title: "Overview | ü¶úüï∏Ô∏èLangGraph",
          priority: null,
          changefreq: null,
          loc: "https://langchain-ai.github.io/langgraph/concepts/low_level/",
          lastmod: "2025-06-26",
          content_type: null,
          uuid: "319a86c5-83ad-58b0-846e-8d8420b95cf7",
        },
        page_content:
          'class State(TypedDict):\n    foo: int\n    bar: Annotated[list[str], add]\n\nIn this example, we\'ve used the Annotated type to specify a reducer function (operator.add) for the second key (bar). Note that the first key remains unchanged. Let\'s assume the input to the graph is {"foo": 1, "bar": ["hi"]}. Let\'s then assume the first Node returns {"foo": 2}. This is treated as an update to the state. Notice that the Node does not need to return the whole State schema - just an update. After applying this update, the State would then be {"foo": 2, "bar": ["hi"]}. If the second node returns {"bar": ["bye"]} then the State would then be {"foo": 2, "bar": ["hi", "bye"]}. Notice here that the bar key is updated by adding the two lists together.\nWorking with Messages in Graph State¬∂\nWhy use messages?¬∂\nMost modern LLM providers have a chat model interface that accepts a list of messages as input. LangChain\'s ChatModel in particular accepts a list of Message objects as inputs. These messages come in a variety of forms such as HumanMessage (user input) or AIMessage (LLM response). To read more about what message objects are, please refer to this conceptual guide.\nUsing Messages in your Graph¬∂\nIn many cases, it is helpful to store prior conversation history as a list of messages in your graph state. To do so, we can add a key (channel) to the graph state that stores a list of Message objects and annotate it with a reducer function (see messages key in the example below). The reducer function is vital to telling the graph how to update the list of Message objects in the state with each state update (for example, when a node sends an update). If you don\'t specify a reducer, every state update will overwrite the list of messages with the most recently provided value. If you wanted to simply append messages to the existing list, you could use operator.add as a reducer.\nHowever, you might also want to manually update messages in your graph state (e.g. human-in-the-loop). If you were to use operator.add, the manual state updates you send to the graph would be appended to the existing list of messages, instead of updating existing messages. To avoid that, you need a reducer that can keep track of message IDs and overwrite existing messages, if updated. To achieve this, you can use the prebuilt add_messages function. For brand new messages, it will simply append to existing list, but it will also handle the updates for existing messages correctly.\nSerialization¬∂\nIn addition to keeping track of message IDs, the add_messages function will also try to deserialize messages into LangChain Message objects whenever a state update is received on the messages channel. See more information on LangChain serialization/deserialization here. This allows sending graph inputs / state updates in the following format:\n# this is supported\n{"messages": [HumanMessage(content="message")]}\n\n# and this is also supported\n{"messages": [{"type": "human", "content": "message"}]}\n\nSince the state updates are always deserialized into LangChain Messages when using add_messages, you should use dot notation to access message attributes, like state["messages"][-1].content. Below is an example of a graph that uses add_messages as it\'s reducer function.\nAPI Reference: AnyMessage | add_messages\nfrom langchain_core.messages import AnyMessage\nfrom langgraph.graph.message import add_messages\nfrom typing import Annotated\nfrom typing_extensions import TypedDict\n\nclass GraphState(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n\nMessagesState¬∂\nSince having a list of messages in your state is so common, there exists a prebuilt state called MessagesState which makes it easy to use messages. MessagesState is defined with a single messages key which is a list of AnyMessage objects and uses the add_messages reducer. Typically, there is more state to track than just messages, so we see people subclass this state and add more fields, like:\nfrom langgraph.graph import MessagesState',
        type: "Document",
      },
      {
        metadata: {
          source: "https://langchain-ai.github.io/langgraph/how-tos/graph-api/",
          language: "",
          description: "",
          title: "Use the Graph API | ü¶úüï∏Ô∏èLangGraph",
          priority: null,
          changefreq: null,
          loc: "https://langchain-ai.github.io/langgraph/how-tos/graph-api/",
          lastmod: "2025-06-26",
          content_type: null,
          uuid: "eb85f53b-7544-571d-b454-d8ded1be6503",
        },
        page_content:
          '================================ Human Message =================================\n\nHi\n================================== Ai Message ==================================\n\nHello!\n\nProcess state updates with reducers¬∂\nEach key in the state can have its own independent reducer function, which controls how updates from nodes are applied. If no reducer function is explicitly specified then it is assumed that all updates to the key should override it.\nFor TypedDict state schemas, we can define reducers by annotating the corresponding field of the state with a reducer function.\nIn the earlier example, our node updated the "messages" key in the state by appending a message to it. Below, we add a reducer to this key, such that updates are automatically appended:\nfrom typing_extensions import Annotated\n\ndef add(left, right):\n    """Can also import `add` from the `operator` built-in."""\n    return left + right\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add]\n    extra_field: int\n\nNow our node can be simplified:\ndef node(state: State):\n    new_message = AIMessage("Hello!")\n    return {"messages": [new_message], "extra_field": 10}\n\nAPI Reference: START\nfrom langgraph.graph import START\n\ngraph = StateGraph(State).add_node(node).add_edge(START, "node").compile()\n\nresult = graph.invoke({"messages": [HumanMessage("Hi")]})\n\nfor message in result["messages"]:\n    message.pretty_print()\n\n================================ Human Message =================================\n\nHi\n================================== Ai Message ==================================\n\nHello!\n\nMessagesState¬∂\nIn practice, there are additional considerations for updating lists of messages:\n\nWe may wish to update an existing message in the state.\nWe may want to accept short-hands for message formats, such as OpenAI format.\n\nLangGraph includes a built-in reducer add_messages that handles these considerations:\nAPI Reference: add_messages\nfrom langgraph.graph.message import add_messages\n\nclass State(TypedDict):\n    messages: Annotated[list[AnyMessage], add_messages]\n    extra_field: int\n\ndef node(state: State):\n    new_message = AIMessage("Hello!")\n    return {"messages": [new_message], "extra_field": 10}\n\ngraph = StateGraph(State).add_node(node).set_entry_point("node").compile()\n\ninput_message = {"role": "user", "content": "Hi"}\n\nresult = graph.invoke({"messages": [input_message]})\n\nfor message in result["messages"]:\n    message.pretty_print()\n\n================================ Human Message =================================\n\nHi\n================================== Ai Message ==================================\n\nHello!\n\nThis is a versatile representation of state for applications involving chat models. LangGraph includes a pre-built MessagesState for convenience, so that we can have:\nfrom langgraph.graph import MessagesState\n\nclass State(MessagesState):\n    extra_field: int\n\nDefine input and output schemas¬∂\nBy default, StateGraph operates with a single schema, and all nodes are expected to communicate using that schema. However, it\'s also possible to define distinct input and output schemas for a graph.\nWhen distinct schemas are specified, an internal schema will still be used for communication between nodes. The input schema ensures that the provided input matches the expected structure, while the output schema filters the internal data to return only the relevant information according to the defined output schema.\nBelow, we\'ll see how to define distinct input and output schema.\nAPI Reference: StateGraph | START | END\nfrom langgraph.graph import StateGraph, START, END\nfrom typing_extensions import TypedDict\n\n# Define the schema for the input\nclass InputState(TypedDict):\n    question: str\n\n# Define the schema for the output\nclass OutputState(TypedDict):\n    answer: str\n\n# Define the overall schema, combining both input and output\nclass OverallState(InputState, OutputState):\n    pass',
        type: "Document",
      },
      {
        metadata: {
          source: "https://python.langchain.com/docs/how_to/message_history/",
          description:
            "This guide assumes familiarity with the following concepts:",
          language: "en",
          title: "How to add message history | ü¶úÔ∏èüîó LangChain",
          priority: "0.5",
          changefreq: "weekly",
          loc: "https://python.langchain.com/docs/how_to/message_history/",
          lastmod: null,
          content_type: null,
          uuid: "43ad36a2-86bf-53f5-af55-03bea9808421",
        },
        page_content:
          'Note that in the below state:\n\n- Updates to the `messages` list will append messages;\n\n- Updates to the `language` string will overwrite the string.\n\n```python\nfrom typing import Sequence\n\nfrom langchain_core.messages import BaseMessage\nfrom langgraph.graph.message import add_messages\nfrom typing_extensions import Annotated, TypedDict\n\nclass State(TypedDict):\n    messages: Annotated[Sequence[BaseMessage], add_messages]\n    language: str\n\nworkflow = StateGraph(state_schema=State)\n\ndef call_model(state: State):\n    response = runnable.invoke(state)\n    # Update message history with response:\n    return {"messages": [response]}\n\nworkflow.add_edge(START, "model")\nworkflow.add_node("model", call_model)\n\nmemory = MemorySaver()\napp = workflow.compile(checkpointer=memory)\n```\n\n**API Reference:**[BaseMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.base.BaseMessage.html) | [add_messages](https://langchain-ai.github.io/langgraph/reference/graphs/#langgraph.graph.message.add_messages)\n```python\nconfig = {"configurable": {"thread_id": "abc345"}}\n\ninput_dict = {\n    "messages": [HumanMessage("Hi, I\'m Bob.")],\n    "language": "Spanish",\n}\noutput = app.invoke(input_dict, config)\noutput["messages"][-1].pretty_print()\n```\n\n```output\n==================================[1m Ai Message [0m==================================\n\n¬°Hola, Bob! Es un placer conocerte.\n```\n\n## Managing message history‚Äã\n\nThe message history (and other elements of the application state) can be accessed via `.get_state`:\n\n```python\nstate = app.get_state(config).values\n\nprint(f\'Language: {state["language"]}\')\nfor message in state["messages"]:\n    message.pretty_print()\n```\n\n```output\nLanguage: Spanish\n================================[1m Human Message [0m=================================\n\nHi, I\'m Bob.\n==================================[1m Ai Message [0m==================================\n\n¬°Hola, Bob! Es un placer conocerte.\n```\n\nWe can also update the state via `.update_state`. For example, we can manually append a new message:\n\n```python\nfrom langchain_core.messages import HumanMessage\n\n_ = app.update_state(config, {"messages": [HumanMessage("Test")]})\n```\n\n**API Reference:**[HumanMessage](https://python.langchain.com/api_reference/core/messages/langchain_core.messages.human.HumanMessage.html)\n```python\nstate = app.get_state(config).values\n\nprint(f\'Language: {state["language"]}\')\nfor message in state["messages"]:\n    message.pretty_print()\n```\n\n```output\nLanguage: Spanish\n================================[1m Human Message [0m=================================\n\nHi, I\'m Bob.\n==================================[1m Ai Message [0m==================================\n\n¬°Hola, Bob! Es un placer conocerte.\n================================[1m Human Message [0m=================================\n\nTest\n```\n\nFor details on managing state, including deleting messages, see the LangGraph documentation:\n\n- [How to delete messages](https://langchain-ai.github.io/langgraph/how-tos/memory/delete-messages/)\n\n- [How to view and update past graph state](https://langchain-ai.github.io/langgraph/how-tos/human_in_the_loop/time-travel/)\n\n#### Was this page helpful?\n\n- [Setup](#setup)\n\n- [Example: message inputs](#example-message-inputs)\n\n- [Example: dictionary inputs](#example-dictionary-inputs)\n\n- [Managing message history](#managing-message-history)',
        type: "Document",
      },
      {
        metadata: {
          loc: "https://langchain-ai.github.io/langgraph/cloud/how-tos/use_stream_react/",
          description: "",
          language: "",
          title: "Integrate LangGraph into a React app | ü¶úüï∏Ô∏èLangGraph",
          priority: null,
          changefreq: null,
          source:
            "https://langchain-ai.github.io/langgraph/cloud/how-tos/use_stream_react/",
          lastmod: "2025-06-26",
          content_type: null,
          uuid: "b7d8bf00-7abe-52ac-83b8-eafb3664fcf9",
        },
        page_content:
          'function EditMessage({\n  message,\n  onEdit,\n}: {\n  message: Message;\n  onEdit: (message: Message) => void;\n}) {\n  const [editing, setEditing] = useState(false);\n\n  if (!editing) {\n    return (\n      <button type="button" onClick={() => setEditing(true)}>\n        Edit\n      </button>\n    );\n  }\n\n  return (\n    <form\n      onSubmit={(e) => {\n        e.preventDefault();\n        const form = e.target as HTMLFormElement;\n        const content = new FormData(form).get("content") as string;\n\n        form.reset();\n        onEdit({ type: "human", content });\n        setEditing(false);\n      }}\n    >\n      <input name="content" defaultValue={message.content as string} />\n      <button type="submit">Save</button>\n    </form>\n  );\n}\n\nexport default function App() {\n  const thread = useStream({\n    apiUrl: "http://localhost:2024",\n    assistantId: "agent",\n    messagesKey: "messages",\n  });\n\n  return (\n    <div>\n      <div>\n        {thread.messages.map((message) => {\n          const meta = thread.getMessagesMetadata(message);\n          const parentCheckpoint = meta?.firstSeenState?.parent_checkpoint;\n\n          return (\n            <div key={message.id}>\n              <div>{message.content as string}</div>\n\n              {message.type === "human" && (\n                <EditMessage\n                  message={message}\n                  onEdit={(message) =>\n                    thread.submit(\n                      { messages: [message] },\n                      { checkpoint: parentCheckpoint }\n                    )\n                  }\n                />\n              )}\n\n              {message.type === "ai" && (\n                <button\n                  type="button"\n                  onClick={() =>\n                    thread.submit(undefined, { checkpoint: parentCheckpoint })\n                  }\n                >\n                  <span>Regenerate</span>\n                </button>\n              )}\n\n              <BranchSwitcher\n                branch={meta?.branch}\n                branchOptions={meta?.branchOptions}\n                onSelect={(branch) => thread.setBranch(branch)}\n              />\n            </div>\n          );\n        })}\n      </div>\n\n      <form\n        onSubmit={(e) => {\n          e.preventDefault();\n\n          const form = e.target as HTMLFormElement;\n          const message = new FormData(form).get("message") as string;\n\n          form.reset();\n          thread.submit({ messages: [message] });\n        }}\n      >\n        <input type="text" name="message" />\n\n        {thread.isLoading ? (\n          <button key="stop" type="button" onClick={() => thread.stop()}>\n            Stop\n          </button>\n        ) : (\n          <button key="submit" type="submit">\n            Send\n          </button>\n        )}\n      </form>\n    </div>\n  );\n}\n\nFor advanced use cases you can use the experimental_branchTree property to get the tree representation of the thread, which can be used to render branching controls for non-message based graphs.\nOptimistic Updates¬∂\nYou can optimistically update the client state before performing a network request to the agent, allowing you to provide immediate feedback to the user, such as showing the user message immediately before the agent has seen the request.\nconst stream = useStream({\n  apiUrl: "http://localhost:2024",\n  assistantId: "agent",\n  messagesKey: "messages",\n});\n\nconst handleSubmit = (text: string) => {\n  const newMessage = { type: "human" as const, content: text };\n\n  stream.submit(\n    { messages: [newMessage] },\n    {\n      optimisticValues(prev) {\n        const prevMessages = prev.messages ?? [];\n        const newMessages = [...prevMessages, newMessage];\n        return { ...prev, messages: newMessages };\n      },\n    }\n  );\n};\n\nTypeScript¬∂\nThe useStream() hook is friendly for apps written in TypeScript and you can specify types for the state to get better type safety and IDE support.\n// Define your types\ntype State = {\n  messages: Message[];\n  context?: Record<string, unknown>;\n};',
        type: "Document",
      },
    ],
  },
};
